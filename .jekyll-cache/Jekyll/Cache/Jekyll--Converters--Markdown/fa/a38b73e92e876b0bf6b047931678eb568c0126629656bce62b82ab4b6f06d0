I"=<p>To my beloved Swimmer pals and strangers: I want to introduce this wonderful
resource named <a href="https://christophm.github.io/interpretable-ml-book/"><strong>Interpretable Machine Learning</strong></a>
by Christoph Molnar. This book has comprehensive concepts about interpretability
on machine learning and deep learning, so I think it’s worth to study together
to understand what the field is and what are happening nowadays.</p>

<p>I will generally follow the chapters of the book from top to bottom, but with
some additional resources which I think could help the understanding of some
concepts. In this pilot page, I’ll go through the motivation and introduction
of this topic with the material of a NeurIPS 2020 tutorial, <em>“Explaining Machine 
Learning Predictions: State-of-the-art, Challenges, and Opportunities”</em> by H.
Lakkaraju et al. You can find the original source from <a href="https://explainml-tutorial.github.io/neurips20">here</a>.</p>

<h2 id="why-do-we-need-interpretable-models">Why do we need interpretable models?</h2>

<p>So why? No one can deny that machine learning is literally everywhere in this
world. Especially from mid-2010 when deep learning models first emerged, almost
all challenging tasks’ SoA solutions have been eplaced by those ominous looking stacked
layers of matrices and functions. Those are deep; deeeep like Mariana Trench.
The only difference between the trench and deep learning models is that the former
has fixed depth but the latter is constantly getting deeper and even bigger.</p>

<p>One similarity I want to stress here is that both are not revealing much of their
essence. We know too little about deep learning. We know that it can solve numerous
tasks, but have no idea for <em>why it does perform well</em>. Unlike ‘traditional’ 
statistical models, deep neural networks’ interior is opaque – cannot find the
meaning at the first sight.</p>

<p>This critical ignorance for the essence of the greatest power the world has is not
welcomed by DARPA<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>. They concerned that when the models’ domain is so delicate,
emboding the sensitive virtues, or related to human life, the slightest error can
bring disasterous consequence. Especially when a human take charge in the
model, they have zero-ish idea why the model performed how.
They denoted this problem as <strong>XAI: eXplainable Artificial Intelligence</strong> and 
declared it as one of the core programs of their branches:</p>
<blockquote>
  <p>XAI is one of a handful of current DARPA programs expected to enable “third-wave AI <br />
systems”, where machines understand the context and environment in which they operate,  …</p>
</blockquote>

<p>From then, the various works related to explainability and interpretability
converged into the newly named field, XAI. We want the full control on what we
make, and benefit us with the reliable results.</p>

<h2 id="case-studies-of-ml-failures">Case studies of ML failures</h2>

<p>Cases are excerpted from Last Week in AI #104<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>.</p>

<h3 id="bayerischer-rundfunks-study">Bayerischer Rundfunk’s study</h3>

<p>You can see the whole media <a href="https://web.br.de/interaktiv/ki-bewerbung/en/">here</a>.
Although AI is believed to introduce less ‘prejudice’ than people do, the German
broadcasting company BR’s study found that in some AI assistants used for assessing
the job seekers’ video are affected by irrelevant features, such as wearing glasses
or headscarf or putting a painting on the wall behind, during the applicants’ 
personality assessment. Surely, that the target model seems not trained ‘fairly’,
but should we note that there is no way to check this fairness unless we do post hoc
examination so far, and moreover, no policy is made for making fair models.</p>

<h3 id="can-computer-algorithms-learn-to-fight-wars-ethically">Can Computer Algorithms Learn to Fight Wars Ethically?</h3>

<p>Not exactly a failure already happened, but this <a href="https://www.washingtonpost.com/magazine/2021/02/17/pentagon-funds-killer-robots-but-ethics-are-under-debate/">WP article</a>
triggers the debate on AIs’ decision making process. How would the robot handle
kids in the war situation? Should they perform inaction to drive people to action
in some cases? AI ethics and XAI are tightly connected in these themes, and to get
any plausible solution of this, certain amount of translucent decision process
is required for the AI.</p>

<h2 id="how-do-we-achieve-model-understanding">How do we achieve model understanding?</h2>

<h3 id="1-build-inherently-interpretable-predictable-models">1. Build inherently interpretable predictable models</h3>

<p>We can use decision tree or linear regression to boost the explainability
of the decision. However, those traditional approaches lack performance.</p>

<h3 id="2-explain-pre-built-models-in-a-post-hoc-manner">2. Explain pre-built models in a post-hoc manner</h3>

<p>Since we usually encounter (deep) neural networks to examine, we need an <em>explainer</em>
to interpret the result of the model. The approaches can be either local or
global, regarding the case’s sensitivity.</p>

<h2 id="wrap-up">Wrap up</h2>

<p>Hope this short post motivated you to start a journey to interpretability of
machine learning. This is a fairly new field yet has utmost importance to our
future scenary:)</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Explainable Artificial Intelligence (XAI) (2018), M. Turek, <a href="https://www.darpa.mil/program/explainable-artificial-intelligence">https://www.darpa.mil/program/explainable-artificial-intelligence</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Last Week in AI #104 (2021), <a href="https://lastweekin.ai/p/104">https://lastweekin.ai/p/104</a> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET